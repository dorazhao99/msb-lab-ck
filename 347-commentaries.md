# CS 347 Commentaries
## Details 
|  Key | Value|
| ------------- | ------------- |
| Description | Details on how to write commentaries for CS347 |

### Commentary Guidelines
The goal of paper commentaries are to get you to think critically about the research that a paper presents and why that research is important. You will write brief commentary reflections—around three to four paragraphs—for each reading in the course.

Writing a strong commentary means grappling with the core ideas being presented in the reading. The goal is not to summarize the paper — everyone reading your commentary will have already read the paper!

Suggested strategies for writing a commentary:

*Don’t*
- Nitpick low-level details
- Harp on already-acknowledged limitations or future work
- Bring expectations from other HCI paper genres (“needs a user study!”)
- Spend too much time summarizing
- Levy judgment (“I like this!”) without digging into the why or the implications of that agreement or disagreement

*Do: engage with the core contributions. To achieve this, we suggest the following three step process:*

- Step 1 (Reflection): Ask yourself, what is the point that this paper is trying to make? (You don’t need to write out the answer to this question in your commentary, but you do need to know the answer in order to write a good commentary.) State the main point briefly, but then reflect on why the ideas in the reading made sense from the authors’ perspectives.
- Step 2 (Synthesis): How effectively does it convince you of that argument? How could the argument be even more persuasive, on its own terms?
- Step 3 (Future work): What are the implications of the argument? Given the ideas presented in the paper, what would you want to work on, or how would you modify those ideas?

Common issues leading to low-scoring commentaries:

- Step 1(Reflection): We get lots of commentaries that are mostly summary. Don’t stop here. We’ve all read the paper. 1–2 sentences max.
- Step 2 (Synthesis): It’s easy to just lob criticisms and negativity. Too many commentaries are just lists of complaints. Instead, focus on: what’s at the core of this idea, and why is it holding sway? What might be a better version of this idea, if you’re unconvinced?
- Step 3 (Future work): Too few commentaries cover this! Instead, ask yourself: what are applications of these ideas, and what follow-up ideas might be worth exploring?
- 
Commentaries are due at 5:00 PM the day before the lecture on Canvas. After 5pm, your commentary will be shared with students in your section so that the discussant can begin work on their commentary synthesis. Late submissions will not be accepted. We will drop the four lowest commentary grades at the end of class: meaning, you may drop four readings’ (not four days’) worth of commentaries.

Commentaries will be graded on a check-minus/check/check-plus scale. The rubric will be:

- Check-minus: Surface-level engagement with the readings, or a repeat of a style of critique that the staff told the class to avoid. Examples of surface-level engagement include: comments about whether the commenter likes or would use the technology, a summary of the paper rather than a reflections on the ideas, or critiques that engage only obliquely with the paper or indicate that the commenter didn’t fully read it. Partially complete submissions may also earn a check-minus if appropriate.
- Check: Effective engagement with the readings. Example commentaries involving check grades often indicate that they understand the main ideas of the papers, and the reflections are reasonably nontrivial observations worth discussing.
- Check-plus: Excellent engagement with the readings. Check-plus grades are reserved for rare instances where a commentary really hits on an interesting, unique, and insightful point of view worth sharing. Generally only a few submissions in each session earn a check-plus.

### Commentary Example
The following example demonstrates the format expected from the commentaries:

I enjoyed learning about how the researchers used different approaches and compared-and-contrasted them in order to see how various tools categorized the importance of different graphical sections. I was most interested by the difference between Ground Truth BubbleView clicks and Predicted Importance projections. Specifically, I thought it was interesting that many of these graphics were magazine-esque layouts, with background images that projected the theme, but often didn’t have super specific information, and large amounts of text in various sizes, colors, and placements. With BubbleView, it seemed that lots of people would click on the text, as well as these background images, but Predicted Importance often thought that the background image wasn’t too important. I though this finding reiterated how important it is to choose background images that are intentional, and not just filler images to make the page important. If people’s attention goes there more than we’d expect, it’s crucial to spend time choosing images that accentuate your points rather than distract the reader.

One experience this reminded me of is other AI tools and algorithms that I have seen on social media sites, specifically those that try to decide the important areas of an image that should be showcased when cropped. Specifically, I remember similar cropping / importance algorithms being used on Twitter / X. However, these tools turned out to be extremely problematic, often cropping the images to focus on those who are white, thin, and female. Many articles dived into understanding this, and users themselves tested by adding photos with underrepresented populations vs. more privileged groups and saw the ethical issues themselves. Many tech people wrote articles (ex. Twitter’s Photo-Cropping Algorithm Favors Young, Thin Females [Links to an external site.]), as well as Twitter themselves (Sharing learnings about our image cropping algorithm [Links to an external site.]). The biases present in this algorithm were deeply connected to general biases in AI, and I would have loved to see the article dive into that possibility more. They do not touch on any biases in their training data, or any edge cases they see that might need further exploration.

Overall, I felt that future work for this article relies on more than just making the tool open source for people to explore themselves. Certain fonts, text patterns, and images grabbed the attention of the user greatly, and a guide to those recommendations would make these learnings even more applicable to the average designer. Although these guides of design tips may exist now, I am sure that at the time of the release of this article, these suggestions would have really changed how people presented media online.
